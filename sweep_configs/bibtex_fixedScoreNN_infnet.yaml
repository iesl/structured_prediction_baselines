name: bibtex_infnet_wFixed_NCE_new_joint
program: allennlp
method: bayes
command:
  - ${program}
  - "train_with_wandb"
  - "model_configs/multilabel_classification/v2.5/fixed_scoreNN/bibtex_pretrained_NCE_v2.jsonnet"
  - "--include-package=structured_prediction_baselines"
  - "--wandb_project=structured_prediction_baselines"
  - "--wandb_entity=score-based-learning"
  - "--wandb_tags=task=mlc,scoreNN=NCE,taskNN=inference_net,dataset=bibtex"
  - ${args}
  - "--file-friendly-logging"  
parameters: "Train TaskNN using scoreNN (pretrained NCE) only or very minimal cross entropy at begginig.."
metric:
  name: best_validation_fixed_f1
  goal: maximize
parameters:
  # Environment variables.
  env.ff_dropout:
    values: [0.5]
  env.ff_hidden:
    values: [400]
  env.ff_linear_layers:
    values: [2]
  env.global_score_hidden_dim:
    values: [200]
  env.cross_entropy_loss_weight:
    distribution: q_uniform
    q: 1 # 0,1
    min: 0 
    max: 1 
  env.inference_score_weight:
    values: [1]
  # env.num_samples:
  #   values: [0]
  # Direct config variables. (sampler --> inference_module)
  env.stopping_criteria:
    distribution: q_uniform
    q: 5 # 1, 5, 10, 15 --> manual handling in the jsonnet.
    min: 0
    max: 15
  trainer.optimizer.optimizers.task_nn.lr:
    distribution: log_uniform
    max: -7
    min: -12
  trainer.optimizer.optimizers.task_nn.weight_decay:
    distribution: log_uniform
    max: -9
    min: -14 
  ## originally. 
  # model.sampler.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -10
  # trainer.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -7
  # trainer.optimizer.weight_decay:
  #   distribution: log_uniform
  #   max: -2
  #   min: -12
