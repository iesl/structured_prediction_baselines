name: expr_fun_margin_l1_tasknn_no_xtropy
description: "Train tasknn using inference score loss (as in Tu & Gimpel) without cross entropy loss. The score-nn will be trained using margin based loss with cost augmented network. There is no sampling in this version"
program: allennlp
command:
- ${program}
- train_with_wandb
- model_configs/multilabel_classification/v2.5/gendata_margin_l1_tasknn_scorestep.jsonnet
- --include-package=structured_prediction_baselines
- --wandb_tags="task=mlc,score_nn_loss=margin,task_nn_loss=bce+inference_score,sampler=inference_net,dataset=expr_fun,inference_module=inference_net,inference_module=tasknn,inference_module_loss=bce+inference_score,sampler=tasknn"
- ${args}
- --file-friendly-logging
method: bayes
metric:
  goal: maximize
  name: "validation/best_fixed_f1"

early_terminate:
  type: hyperband
  min_iter: 20

parameters:
  env.dataset_name:
    value: 'expr_fun'
  env.cross_entropy_loss_weight:
    value: 0
  env.inference_score_weight:
    distribution: log_uniform
    min: -6.9
    max: 2.3
  env.ff_dropout_10x:
    value: 2
  env.ff_hidden:
    value: 200
  env.ff_linear_layers:
    value: 4
  env.ff_weight_decay:
    value: 0.00001
  env.global_score_hidden_dim:
    distribution: q_uniform
    min: 50  #100
    max: 449 #400
    q: 100
  trainer.optimizer.optimizers.task_nn.lr:
    distribution: log_uniform
    min: -12.5
    max: -4.5
  trainer.optimizer.optimizers.score_nn.lr:
    distribution: log_uniform
    min: -11.5
    max: -4.5
  env.task_nn_steps: # instead of trainer.num_steps.task_nn
    distribution: q_uniform
    q: 5 # 1, 5, 10 --> manual handling in the jsonnet.
    min: 0
    max: 10
  env.score_nn_steps:
    distribution: q_uniform
    min: 0
    max: 11.99
    q: 3
