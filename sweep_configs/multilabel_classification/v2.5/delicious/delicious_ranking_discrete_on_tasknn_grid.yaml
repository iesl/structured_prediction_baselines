name: delicious_nce_discrete_minus_tasknn_reverse
description: "Train tasknn using cross-entropy and  score loss (v(f(x),y)). The score-nn will be trained using NCE with a - sign (score-ln Pn). The samples are taken as discrete samples from the tasknn output."
program: allennlp
command:
- ${program}
- train_with_wandb
- model_configs/multilabel_classification/v2.5/gendata_ranking_discrete_on_tasknn.jsonnet
- --include-package=structured_prediction_baselines
- --wandb_tags="task=mlc,model=dvn,sampler=inference_net_continuous_samples,dataset=delicious,inference_module=inference_net,inference_module=tasknn,sampler=tasknn_contiuous_samples"
- ${args}
- --file-friendly-logging
method: grid
metric:
  goal: maximize
  name: "validation/best_fixed_f1"

early_terminate:
  type: hyperband
  min_iter: 20

# you need total of 3*5 = 15 runs.
# make 5 agents & run 3 runs each.
parameters:
  env.dataset_name:
    value: 'delicious'
  env.ff_dropout_10x:
    value: 5
  env.ff_hidden:
    value: 400
  env.ff_linear_layers:
    value: 2
  env.ff_weight_decay:
    value: 0.00001
  env.global_score_hidden_dim:
    value: 200
  env.cross_entropy_loss_weight:
    value: 1.0
  env.dvn_score_loss_weight:
    values: [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0]
  trainer.optimizer.optimizers.task_nn.lr:
    values: [0.001, 0.005, 0.01]
  env.task_nn_steps: # instead of trainer.num_steps.task_nn
    value: 1
  env.num_samples:
    values: [20, 40, 60, 80, 100]