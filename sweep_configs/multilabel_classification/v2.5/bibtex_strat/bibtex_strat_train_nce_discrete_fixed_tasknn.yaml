name: bibtex_strat_train_nce_discrete_fixed_tasknn
description: "Train tasknn using cross-entropy and  (unnormalized) score loss (v(f(x),y)). The score-nn will be trained using NCE with a - sign (score-ln Pn). The samples are taken as discrete samples from the tasknn output."
program: allennlp
command:
- ${program}
- train_with_wandb
- model_configs/multilabel_classification/v2.5/gendata_nce_discrete_fixed_tasknn.jsonnet
- --include-package=structured_prediction_baselines
- --wandb_tags="task=mlc,score_nn_loss=nce,task_nn_loss=bce+score,sampler=inference_net_continuous_samples,dataset=bibtex_strat,inference_module=inference_net,inference_module=tasknn,sampler=tasknn_contiuous_samples"
- ${args}
- --file-friendly-logging
method: bayes
metric:
  goal: maximize
  name: "validation/best_fixed_f1"

early_terminate:
  type: hyperband
  min_iter: 20

parameters:
  env.dataset_name:
    value: 'bibtex_strat'
  env.ff_dropout_10x:
    value: 5.0
  env.ff_hidden:
    value: 400
  env.ff_linear_layers:
    value: 2
  env.ff_weight_decay:
    value: 0.00001
  env.global_score_hidden_dim:
    value: 200
  # trainer.optimizer.optimizers.task_nn.lr: # --> not used. (commenting to make it noticeable.)
  #   distribution: log_uniform
  #   min: -12.5
  #   max: -4.5 
  trainer.optimizer.optimizers.score_nn.lr:
    distribution: log_uniform
    min: -11.5
    max: -4.5
  model.loss_fn.num_samples:
    distribution: q_uniform
    q: 20 # 10, 25, ..., 50
    min: 20 # ln(10)
    max: 100 # ln(50)
  # model.loss_fn.sign: # --> fixed to be minus. (commenting to make it noticeable.)
  #   value: "-"
