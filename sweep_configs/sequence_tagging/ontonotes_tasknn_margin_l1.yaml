name: ontonotes_margin_l1_tasknn
description: "Train tasknn using cross-entropy and inference score loss (as in Tu & Gimpel). The score-nn will be trained using margin based loss with cost augmented network. There is no sampling in this version"
program: allennlp
command:
- ${program}
- train_with_wandb
- model_configs/sequence_tagging/ontonotes_inference_net_stacked.jsonnet
- --include-package=structured_prediction_baselines
- --wandb_tags="task=srl,score_nn_loss=margin,task_nn_loss=bce+inference_score,sampler=inference_net,dataset=ontonotes,inference_module=inference_net,inference_module=tasknn,inference_module_loss=bce+inference_score,sampler=tasknn"
- ${args}
- --file-friendly-logging
method: grid
metric:
  goal: maximize
  name: "validation/best_f1-measure-overall"

early_terminate:
  type: hyperband
  min_iter: 20

parameters:
  data_loader.batch_sampler.batch_size:
    value: 16
  env.attention_dim:
    values:
      - 128
      - 256
  env.attention_dropout_10x:
    values:
      - 1
      - 2
  env.cross_entropy_loss_weight:
    value: 1
  env.inference_score_weight:
    values:
      - 0.01
      - 0.1
      - 0.2
  env.ff_dropout_10x:
    values:
      - 1
      - 3
  env.ff_hidden:
    values:
      - 256
      - 512
  env.ff_linear_layers:
    values:
      - 4
      - 3
      - 2
    # value: 2 # bibtex
  env.ff_weight_decay:
    value: 0.00001
  trainer.optimizer.optimizers.task_nn.lr:
    values:
      - 0.00001
      - 0.0001
  trainer.optimizer.optimizers.score_nn.lr:
    values:
      - 0.00001
      - 0.0001
