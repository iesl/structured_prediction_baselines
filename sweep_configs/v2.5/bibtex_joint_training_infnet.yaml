name: bibtex_infnet_pretrainedNCE+jointTrain
program: allennlp
method: bayes
command:
  - ${program}
  - "train_with_wandb"
  - "model_configs/multilabel_classification/v2.5/bibtex_nce_discrete_tasknn_reverse_loadNCE.jsonnet"
  - "--include-package=structured_prediction_baselines"
  - "--wandb_project=structured_prediction_baselines"
  - "--wandb_entity=score-based-learning"
  - "--wandb_tags=task=mlc,scoreNN=NCEpretrained,taskNN=inference_net,dataset=bibtex"
  - ${args}
  - "--file-friendly-logging"  
parameters: "Jointly train TaskNN & ScoreNN starting with pretrained ScoreNN (NCE)"
metric:
  name: best_validation_fixed_f1
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 20
parameters:
  # Environment variables.
  env.ff_dropout:
    values: [0.5]
  env.ff_hidden:
    values: [400]
  env.ff_linear_layers:
    values: [2]
  env.global_score_hidden_dim:
    values: [200]
  env.cross_entropy_loss_weight:
    values: [1]
  env.inference_score_weight:
    max: 2
    min: 0
    distribution: uniform
  # env.num_samples:
  #   values: [0]
  # Direct config variables. (sampler --> inference_module)
  env.stopping_criteria:
    distribution: q_uniform
    q: 5 # 1, 5, 10, 15 --> manual handling in the jsonnet.
    min: 0
    max: 15
  trainer.optimizer.optimizers.task_nn.lr:
    distribution: log_uniform
    max: -7
    min: -12
  trainer.optimizer.optimizers.task_nn.weight_decay:
    distribution: log_uniform
    max: -9
    min: -14 
  trainer.optimizer.optimizers.score_nn.lr:
    distribution: log_uniform
    max: -3
    min: -11
  trainer.optimizer.optimizers.score_nn.weight_decay:
    distribution: log_uniform
    max: -9
    min: -14 

  ## originally. 
  # model.sampler.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -10
  # trainer.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -7
  # trainer.optimizer.weight_decay:
  #   distribution: log_uniform
  #   max: -2
  #   min: -12
