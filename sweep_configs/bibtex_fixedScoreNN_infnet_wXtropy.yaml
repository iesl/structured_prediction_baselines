name: bibtex_infnet_wFixed_revNCE_joint+Xtropy
program: allennlp
method: bayes
command:
  - ${program}
  - "train_with_wandb"
  - "model_configs/multilabel_classification/v2.5/fixed_scoreNN/bibtex_pretrained_revNCE_wXtropy.jsonnet"
  - "--include-package=structured_prediction_baselines"
  - "--wandb_project=structured_prediction_baselines"
  - "--wandb_entity=score-based-learning"
  - "--wandb_tags=task=mlc,scoreNN=revNCE,taskNN=inference_net,dataset=bibtex"
  - ${args}
  - "--file-friendly-logging"  
parameters: "Train TaskNN using fixed scoreNN (joitly pretrained revNCE) + Xtropy "
metric:
  name: best_validation_fixed_f1
  goal: maximize
parameters:
  # Environment variables.
  env.ff_dropout:
    values: [0.5]
  env.ff_hidden:
    values: [400]
  env.ff_linear_layers:
    values: [2]
  env.global_score_hidden_dim:
    values: [200]
  env.cross_entropy_loss_weight:
    values: [1]
  env.inference_score_weight:
    max: 2
    min: 0
    distribution: uniform
  # env.num_samples:
  #   values: [0]
  # Direct config variables. (sampler --> inference_module)
  env.stopping_criteria:
    distribution: q_uniform
    q: 5 # 1, 5, 10, 15 --> manual handling in the jsonnet.
    min: 5
    max: 15
  model.inference_module.optimizer.lr:
    distribution: log_uniform
    max: -9
    min: -12
  model.inference_module.optimizer.weight_decay:
    distribution: log_uniform
    max: -9
    min: -14 
  ## originally. 
  # model.sampler.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -10
  # trainer.optimizer.lr:
  #   distribution: log_uniform
  #   max: 0
  #   min: -7
  # trainer.optimizer.weight_decay:
  #   distribution: log_uniform
  #   max: -2
  #   min: -12
